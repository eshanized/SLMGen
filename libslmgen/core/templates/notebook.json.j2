{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Fine-Tune {{ model_name }} with Unsloth\n",
        "\n",
        "**Generated by SLMGEN** | {{ timestamp }}\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Overview\n",
        "- **Examples:** {{ num_examples }}\n",
        "- **Task:** {{ task_type }}\n",
        "- **Model:** {{ model_name }} ({{ model_id }})\n",
        "- **Estimated Time:** ~{{ training_time }} minutes on T4 GPU\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Start\n",
        "1. Click **Runtime ‚Üí Run all** to start training\n",
        "2. Wait for training to complete (~{{ training_time }} min)\n",
        "3. Test your model in the inference section\n",
        "4. Download the LoRA adapter from the files panel\n",
        "\n",
        "> üí° **Tip:** This notebook is self-contained. Your dataset is embedded below - no file uploads needed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Install Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Unsloth - this gives us 2x faster training and 70% less VRAM\n",
        "# Takes about 2-3 minutes on first run\n",
        "\n",
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes triton\n",
        "\n",
        "print(\"‚úÖ Unsloth installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Make sure we have a GPU\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"‚ùå No GPU found! Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "print(f\"‚úÖ GPU: {gpu_name}\")\n",
        "print(f\"‚úÖ Memory: {gpu_mem:.1f} GB\")"
      ]
    },
    {% if is_gated %}
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîê HuggingFace Login\n",
        "\n",
        "This model is gated - you need to:\n",
        "1. Get a HuggingFace token from https://huggingface.co/settings/tokens\n",
        "2. Accept the model license at the model page\n",
        "3. Paste your token below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your HF token here or use the prompt\n",
        "# Get one at: https://huggingface.co/settings/tokens\n",
        "login()"
      ]
    },
    {% endif %}
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Load Your Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "import json\n",
        "\n",
        "# Your dataset is embedded here as base64 - no file uploads needed!\n",
        "DATASET_B64 = \"{{ dataset_b64 }}\"\n",
        "\n",
        "# Decode the dataset\n",
        "dataset_str = base64.b64decode(DATASET_B64).decode()\n",
        "raw_data = [json.loads(line) for line in dataset_str.strip().split(\"\\n\") if line.strip()]\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(raw_data):,} training examples\")\n",
        "\n",
        "# Preview first example\n",
        "print(\"\\nüìù First example:\")\n",
        "print(json.dumps(raw_data[0], indent=2)[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Load Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load with 4-bit quantization for efficient Training\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"{{ model_id }}\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,  # auto-detect\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Loaded {model.config.name_or_path}\")\n",
        "print(f\"‚úÖ Max sequence length: 2048\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Configure LoRA Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add LoRA adapters for efficient fine-tuning\n",
        "# This only trains ~1-5% of the parameters\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank - higher = more capacity but slower\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,  # 0 is optimized\n",
        "    target_modules={{ lora_targets }},\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # saves VRAM\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LoRA adapters configured\")\n",
        "print(f\"‚úÖ Trainable parameters: {model.print_trainable_parameters()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Format Data for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Convert to chat format that the model expects\n",
        "def format_conversation(example):\n",
        "    # Apply the model's chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False,\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_list(raw_data)\n",
        "dataset = dataset.map(format_conversation)\n",
        "\n",
        "print(f\"‚úÖ Formatted {len(dataset):,} examples\")\n",
        "print(\"\\nüìù Formatted example preview:\")\n",
        "print(dataset[0][\"text\"][:800])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è Train the Model\n",
        "\n",
        "This will take approximately **{{ training_time }} minutes** on a T4 GPU.\n",
        "\n",
        "Watch the loss decrease - that means the model is learning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # can be True for short conversations\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=42,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "print(f\"Training loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"Training time: {trainer_stats.metrics['train_runtime'] / 60:.1f} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Save LoRA Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save just the LoRA adapter (small file, ~50-100MB)\n",
        "model.save_pretrained(\"lora_adapter\")\n",
        "tokenizer.save_pretrained(\"lora_adapter\")\n",
        "\n",
        "print(\"‚úÖ Saved LoRA adapter to 'lora_adapter/' folder\")\n",
        "print(\"üí° Download it from the Files panel on the left\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Test Your Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test with a sample prompt - edit this!\n",
        "test_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello! How can you help me today?\"}\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    test_messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"ü§ñ Model response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Export Options\n",
        "\n",
        "Choose how you want to export your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTION 1: Save as GGUF for llama.cpp / Ollama\n",
        "# Uncomment to use:\n",
        "\n",
        "# model.save_pretrained_gguf(\"model-gguf\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "# print(\"‚úÖ Saved GGUF model - ready for Ollama!\")\n",
        "\n",
        "# OPTION 2: Merge LoRA and save full model\n",
        "# Uncomment to use:\n",
        "\n",
        "# model.save_pretrained_merged(\"model-merged\", tokenizer, save_method=\"merged_16bit\")\n",
        "# print(\"‚úÖ Saved merged 16-bit model\")\n",
        "\n",
        "# OPTION 3: Push to HuggingFace Hub\n",
        "# Uncomment and edit:\n",
        "\n",
        "# model.push_to_hub(\"your-username/model-name\", token=\"your-hf-token\")\n",
        "# tokenizer.push_to_hub(\"your-username/model-name\", token=\"your-hf-token\")\n",
        "# print(\"‚úÖ Pushed to HuggingFace Hub!\")\n",
        "\n",
        "print(\"üí° Uncomment the export option you want to use above!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ What's Next?\n",
        "\n",
        "Your fine-tuned model is ready! Here's what you can do:\n",
        "\n",
        "1. **Download the LoRA adapter** from the `lora_adapter/` folder\n",
        "2. **Export to GGUF** for use with Ollama or llama.cpp\n",
        "3. **Push to HuggingFace** to share your model\n",
        "4. **Test more prompts** to make sure it works well\n",
        "\n",
        "---\n",
        "\n",
        "*Generated with ‚ù§Ô∏è by [SLMGEN](https://github.com/eshanized/slmgen)*"
      ]
    }
  ]
}
